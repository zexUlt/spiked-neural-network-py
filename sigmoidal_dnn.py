# -*- coding: utf-8 -*-
"""Sigmoidal DNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16XvrPIf2qbDt_9okA4Ur4SvC5aXCBpWQ
"""

import numpy as np
import matplotlib.pyplot as plt
from activation_functions import Sigmoid, Izhikevich
from SpikedNN import SpikedNN
from typing import Tuple
from sklearn.model_selection import GridSearchCV


"""# Initial system"""
def aa(t):
    res = np.array([[np.cos(t), np.sin(t)],
                    [-np.sin(t), np.cos(t)]])
    return np.array(res)


def bb(t):
    return np.array([[np.cos(t), 0],
                     [0, np.sin(t)]])


def uu(t):
    return np.array([[np.exp(np.sin(t)), np.exp(np.sin(t)) * (1 - np.cos(t) + np.sin(t))]]).T


def xx(t):
    return np.array([[np.exp(np.sin(t)) * (1 - np.cos(t) + np.sin(t)), np.exp(np.sin(t))]]).T


def initialize_base_vars():
    time_end = 6 * np.pi
    step = 1e-4
    time = np.arange(0, time_end, step)
    x_in = xx(time)
    u_in = uu(time)

    return time_end, step, x_in, u_in, time


def draw_smth(time: np.array, x_in: np.array, x_pred: np.array, loss: np.array):
    # noinspection PyTypeChecker
    fig, ax = plt.subplots(ncols=2, figsize=(15, 3), sharex=True, sharey=False)
    ax[0].plot(time, x_in[:, 0, :], label=f"x0 input")
    ax[0].plot(time, x_pred[:, 0, :], label=f"x0 output")
    ax[0].set_title('x0')
    ax[0].legend()
    # ax[0].set_ylim([-6, 6])

    ax[1].plot(time, x_in[:, 1, :], label=f"x1 input")
    ax[1].plot(time, x_pred[:, 1, :], label=f"x1 output")
    ax[1].set_title('x1')
    ax[1].legend()
    # ax[1].set_ylim([-6, 3.3])
    plt.show()

    # noinspection PyTypeChecker
    fig, ax = plt.subplots(ncols=2, figsize=(15, 2), sharex=True, sharey=False)
    ax[0].plot(time[:-1], loss[:, 0, :], label=f"loss1")
    ax[0].legend()
    ax[1].plot(time[:-1], loss[:, 1, :], label=f"loss2")
    ax[1].legend()
    plt.show()


def get_variables_shapes(dim1, dim2, state_size, u_size):
    b1 = dim1
    c1 = (state_size, dim1)
    d1 = dim1
    e1 = dim1

    b2 = (dim2, u_size)
    c2 = (state_size, dim2, u_size)
    d2 = (dim2, u_size)
    e2 = (dim2, u_size)

    return b1, c1, d1, e1, b2, c2, d2, e2


def plot_initial_system():
    xlim = 2 * np.pi
    # xlim = time_end

    dx = np.array([aa(t) @ xx(t) + bb(t) @ uu(t) for t in time])

    a = np.array([aa(t) for t in time])
    a_eigs = np.linalg.eig(a)[0]

    b = np.array([bb(t) for t in time])
    b_eigs = np.linalg.eig(b)[0]

    init_fig, init_ax = plt.subplots(ncols=2, figsize=(15, 3), sharex='all', sharey='none')
    init_ax[0].plot(time, x_input[:, 0, :])  # , label=f"lr={res.learning_rate}")
    init_ax[0].set_title('x0')
    # ax[0].legend()
    init_ax[1].plot(time, x_input[:, 1, :])  # , label=f"lr={res.learning_rate}")
    init_ax[1].set_title('x1')
    # ax[1].legend()
    init_ax[0].set_xlim(0, xlim)
    init_ax[1].set_xlim(0, xlim)
    init_fig.text(0.05, 0.5, 'X input', ha='center', va='center', rotation='vertical', fontsize=14)
    plt.show()

    init_fig, init_ax = plt.subplots(ncols=2, figsize=(15, 3), sharex='all', sharey='none')
    init_ax[0].plot(time, u_input[:, 0, :])  # , label=f"lr={res.learning_rate}")
    init_ax[0].set_title('u0')
    # ax[0].legend()
    init_ax[1].plot(time, u_input[:, 1, :])  # , label=f"lr={res.learning_rate}")
    init_ax[1].set_title('u1')
    # ax[1].legend()
    init_ax[0].set_xlim(0, xlim)
    init_ax[1].set_xlim(0, xlim)
    init_fig.text(0.05, 0.5, 'U input', ha='center', va='center', rotation='vertical', fontsize=14)
    plt.show()

    init_fig, init_ax = plt.subplots(ncols=2, figsize=(15, 3), sharex='all', sharey='none')
    init_ax[0].plot(time, dx[:, 0, :])  # , label=f"lr={res.learning_rate}")
    init_ax[0].set_title('dx0')
    # ax[0].legend()
    init_ax[1].plot(time, dx[:, 1, :])  # , label=f"lr={res.learning_rate}")
    init_ax[1].set_title('dx1')
    # ax[1].legend()
    init_ax[0].set_xlim(0, xlim)
    init_ax[1].set_xlim(0, xlim)
    init_fig.text(0.05, 0.5, 'dX input', ha='center', va='center', rotation='vertical', fontsize=14)
    plt.show()

    init_fig, init_ax = plt.subplots(ncols=2, figsize=(15, 3), sharex='all', sharey='none')
    init_ax[0].plot(time, a_eigs[:, 0])
    init_ax[0].set_title('A_eigs0')
    # ax[0].legend()
    init_ax[1].plot(time, a_eigs[:, 1])
    init_ax[1].set_title('A_eigs1')
    # ax[1].legend()
    init_ax[0].set_xlim(0, xlim)
    init_ax[1].set_xlim(0, xlim)
    init_fig.text(0.05, 0.5, 'A eigenvalues', ha='center', va='center', rotation='vertical', fontsize=14)
    plt.show()

    init_fig, init_ax = plt.subplots(ncols=2, figsize=(15, 3), sharex='all', sharey='none')
    init_ax[0].plot(time, b_eigs[:, 0])
    init_ax[0].set_title('B_eigs0')
    # ax[0].legend()
    init_ax[1].plot(time, b_eigs[:, 1])
    init_ax[1].set_title('B_eigs1')
    # ax[1].legend()
    init_ax[0].set_xlim(0, xlim)
    init_ax[1].set_xlim(0, xlim)
    init_fig.text(0.05, 0.5, 'B eigenvalues', ha='center', va='center', rotation='vertical', fontsize=14)
    plt.show()


def run_test_1(params: Tuple):
    np.random.seed(2)

    # b1 = np.random.random(b1_shape) * 10
    # c1 = np.random.random(c1_shape) * 10
    # d1 = np.random.random(d1_shape) * 10
    # e1 = np.zeros(e1_shape)
    # b2 = np.random.random(b2_shape) * 10
    # c2 = np.random.random(c2_shape) * 10
    # d2 = np.random.random(d2_shape) * 10
    # e2 = np.zeros(e2_shape)

    # sigma = Sigmoid(nn1, 1, b1, c1, d1, e1)
    # phi = Sigmoid(nn2, u_size, b2, c2, d2, e2)
    sigma = Izhikevich((nn1, 1), in_scale=params[0], out_scale=params[1])
    phi = Izhikevich((nn2, u_size), in_scale=params[0], out_scale=params[1])

    dnn_params = {
        'state_size': state_size,
        'u_size': u_size,
        'k1': 10,
        'k2': 10,
        'nn1': nn1,
        'nn2': nn2,
        'wa_init': np.random.rand(state_size, nn1) * 1e-1,
        'wb_init': np.random.rand(u_size, nn2) * 1e-1,
        'a_init': np.identity(2) * -0.99,
        'p_init': np.array([[10.0, 0.0], [0.0, 40.0]]) * 10,
        # 'stop_time': 1*2*np.pi,
        'stop_time': time_end,
        'sigma': sigma,
        'phi': phi,
        'use_x_hat': False
    }

    s_dnn = SpikedNN(**dnn_params)
    # stop = int(0.35*len(x_input))
    # result = s_dnn.run(x=x_input[:stop], u=u_input[:stop], step_size=step)
    result = s_dnn.run(x=x_input, u=u_input, step_size=step)

    print()
    print(result)

    loss_norm = np.sum(np.linalg.norm(s_dnn.history_loss, axis=1))
    loss_delta = np.sum(list(map(lambda el: np.transpose(el) @ el, s_dnn.history_loss)))

    print(f"Loss Norm:\t{loss_norm}")
    print(f"Loss Delta:\t{loss_delta}")

    x = np.array(s_dnn.history_x)
    loss = np.array(s_dnn.history_loss)

    draw_smth(time=time, x_in=x_input, x_pred=x, loss=loss)

    # noinspection PyTypeChecker
    fig, ax = plt.subplots(nrows=max(s_dnn.nn1, s_dnn.nn2), ncols=2, figsize=(15, 5), sharex=True, sharey=False)
    s_dnn_sigma = np.array(s_dnn.history_sigma)
    s_dnn_phi_u = np.array(s_dnn.history_phi_U)

    # sigma
    for i in range(s_dnn.nn1):
        ax[i, 0].plot(time[:-1], s_dnn_sigma[:, i])
        ax[i, 0].set_title(f'$\\sigma_{{{i}}} (x) $')
        # ax[i, 0].set_xlim(0,xlim)

    # phi
    for i in range(s_dnn.nn2):
        ax[i, 1].plot(time[:-1], s_dnn_phi_u[:, i])
        ax[i, 1].set_title(f'$\\varphi_{{{i}}} (x) u$')
        # ax[i, 1].set_xlim(0,xlim)

    plt.show()


def run_test_2(params: Tuple):
    sigma = Izhikevich((nn1, 1), in_scale=params[0], out_scale=params[1])
    phi = Izhikevich((nn2, u_size), in_scale=params[0], out_scale=params[1])
    sigmoidal_dnn_params = {
        'state_size': state_size,
        'u_size': u_size,
        'k1': 10,
        'k2': 10,
        'nn1': nn1,
        'nn2': nn2,
        'wa_init': np.random.rand(state_size, nn1) * 1e-1,
        'wb_init': np.random.rand(u_size, nn2) * 1e-1,
        'a_init': np.identity(2) * -0.99,
        'p_init': np.array([[10.0, 0.0], [0.0, 40.0]]) * 10,
        # 'stop_time': 1*2*np.pi,
        'stop_time': time_end * 0.9,
        'sigma': sigma,
        'phi': phi,
        'use_x_hat': False
    }
    s_dnn = SpikedNN(**sigmoidal_dnn_params)
    # stop = int(0.35*len(x_input))
    # result = s_dnn.run(x=x_input[:stop], u=u_input[:stop], step_size=step)
    result = s_dnn.run(x=x_input, u=u_input, step_size=step)
    print()
    print(result)
    loss_norm = np.sum(np.linalg.norm(s_dnn.history_loss, axis=1))
    loss_delta = np.sum(list(map(lambda el: np.transpose(el) @ el, s_dnn.history_loss)))
    print(f"Loss Norm:\t{loss_norm}")
    print(f"Loss Delta:\t{loss_delta}")
    x = np.array(s_dnn.history_x)
    loss = np.array(s_dnn.history_loss)
    draw_smth(time, x_input, x, loss)


def tune(in_scale_params: Tuple[float, float, float], out_scale_params: Tuple[float, float, float]) -> Tuple:
    in_scale_start, in_scale_end, in_scale_step = in_scale_params
    out_scale_start, out_scale_end, out_scale_step = out_scale_params
    best_score = float('inf')
    best_params = ...

    tune_params = {}

    for in_scale in np.arange(in_scale_start, in_scale_start + in_scale_step, in_scale_step, dtype=float):
        for out_scale in np.arange(out_scale_start, out_scale_start + 10 * out_scale_step, out_scale_step, dtype=float):
            print(f'Tuned params: in_scale={in_scale}, out_scale={out_scale}')

            sigma = Izhikevich((nn1, 1), in_scale=in_scale, out_scale=out_scale)
            phi = Izhikevich((nn2, u_size), in_scale=in_scale, out_scale=out_scale)
            sigmoidal_dnn_params = {
                'state_size': state_size,
                'u_size': u_size,
                'k1': 10,
                'k2': 10,
                'nn1': nn1,
                'nn2': nn2,
                'wa_init': np.random.rand(state_size, nn1) * 1e-1,
                'wb_init': np.random.rand(u_size, nn2) * 1e-1,
                'a_init': np.identity(2) * -0.99,
                'p_init': np.array([[10.0, 0.0], [0.0, 40.0]]) * 10,
                # 'stop_time': 1*2*np.pi,
                'stop_time': time_end * 0.9,
                'sigma': sigma,
                'phi': phi,
                'use_x_hat': False
            }
            s_dnn = SpikedNN(**sigmoidal_dnn_params)
            result = s_dnn.run(x=x_input, u=u_input, step_size=step)

            # print(f'\n{result}')

            loss_norm = np.sum(np.linalg.norm(s_dnn.history_loss, axis=1))
            loss_delta = np.sum(list(map(lambda el: np.transpose(el) @ el, s_dnn.history_loss)))

            print(f"Loss Norm:\t{loss_norm}")
            print(f"Loss Delta:\t{loss_delta}\n")

            if loss_norm < best_score:
                best_params = (in_scale, out_scale)
                best_score = loss_norm

    print(f'Best score: {best_score}', f'Best params: {best_params}', end='\n')
    return best_params


time_end = ...
x_input = ...
u_input = ...
step = ...
time = ...

if __name__ == '__main__':
    nn1 = 2
    nn2 = 2
    state_size = 2
    u_size = 2

    # b1_shape, c1_shape, d1_shape, e1_shape, \
    # b2_shape, c2_shape, d2_shape, e2_shape = get_variables_shapes(dim1=nn1, dim2=nn2,
    #                                                               state_size=state_size,
    #                                                               u_size=u_size)

    time_end, step, x_input, u_input, time = initialize_base_vars()

    best_params = (1., 0.26)
    # best_params = tune((1., 100., 5.), (0.01, 1., 0.05))

    # plot_initial_system()

    run_test_1(best_params)

    run_test_2(best_params)

    # nn1 = 7
    # nn2 = 9
    # state_size = 2
    # u_size = 2
    #
    # b1_shape, c1_shape, d1_shape, e1_shape, b2_shape, c2_shape, d2_shape, e2_shape = get_variables_shapes(nn1=nn1,
    #                                                                                                       nn2=nn2,
    #                                                                                                       state_size=state_size,
    #                                                                                                       u_size=u_size)
    #
    # np.random.seed(2)
    #
    # b1 = np.random.random(b1_shape) * 10
    # c1 = np.random.random(c1_shape) * 10
    # d1 = np.random.random(d1_shape) * 10
    # e1 = np.zeros(e1_shape)
    # b2 = np.random.random(b2_shape) * 10
    # c2 = np.random.random(c2_shape) * 10
    # d2 = np.random.random(d2_shape) * 10
    # e2 = np.zeros(e2_shape)
    #
    # # sigma = Sigmoid(nn1, 1, b1, c1, d1, e1)
    # # phi = Sigmoid(nn2, u_size, b2, c2, d2, e2)
    # sigma = Izhikevich((nn1, 1))
    # phi = Izhikevich((nn2, u_size))
    #
    # sigmoidal_dnn_params = {
    #     'state_size': state_size,
    #     'u_size': u_size,
    #     'k1': 10,
    #     'k2': 10,
    #     'nn1': nn1,
    #     'nn2': nn2,
    #     'wa_init': np.random.rand(state_size, nn1) * 1e-1,
    #     'wb_init': np.random.rand(u_size, nn2) * 1e-1,
    #     'a_init': np.identity(2) * -0.99,
    #     'p_init': np.array([[10.0, 0.0], [0.0, 40.0]]) * 10,
    #     # 'stop_time': 1*2*np.pi,
    #     'stop_time': time_end,
    #     'sigma': sigma,
    #     'phi': phi,
    #     'use_x_hat': False
    # }
    #
    # s_dnn = SpikedNN(**sigmoidal_dnn_params)
    # # stop = int(0.35*len(x_input))
    # # result = s_dnn.run(x=x_input[:stop], u=u_input[:stop], step_size=step)
    # result = s_dnn.run(x=x_input, u=u_input, step_size=step)
    #
    # print()
    # print(result)
    #
    # loss_norm = np.sum(np.linalg.norm(s_dnn.history_loss, axis=1))
    # loss_delta = np.sum(list(map(lambda el: np.transpose(el) @ el, s_dnn.history_loss)))
    #
    # print(f"Loss Norm:\t{loss_norm}")
    # print(f"Loss Delta:\t{loss_delta}")
    #
    # x = np.array(s_dnn.history_x)
    # loss = np.array(s_dnn.history_loss)
    #
    # fig, ax = plt.subplots(ncols=2, figsize=(15, 3), sharex=True, sharey=False)
    # ax[0].plot(time, x_input[:, 0, :], label=f"x0 input")
    # ax[0].plot(time, x[:, 0, :], label=f"x0 output")
    # ax[0].set_title('x0')
    # ax[0].legend()
    # # ax[0].set_ylim([-6, 6])
    #
    # ax[1].plot(time, x_input[:, 1, :], label=f"x1 input")
    # ax[1].plot(time, x[:, 1, :], label=f"x1 output")
    # ax[1].set_title('x1')
    # ax[1].legend()
    # # ax[1].set_ylim([-6, 3.3])
    # plt.show()
    #
    # fig, ax = plt.subplots(ncols=2, figsize=(15, 2), sharex=True, sharey=False)
    # ax[0].plot(time[:-1], loss[:, 0, :], label=f"loss1")
    # ax[0].legend()
    # ax[1].plot(time[:-1], loss[:, 1, :], label=f"loss2")
    # ax[1].legend()
    # plt.show()
    #
    # fig, ax = plt.subplots(nrows=max(s_dnn.nn1, s_dnn.nn2), ncols=2, figsize=(15, 5), sharex=True, sharey=False)
    # s_dnn_sigma = np.array(s_dnn.history_sigma)
    # s_dnn_phi_u = np.array(s_dnn.history_phi_U)
    #
    # # sigma
    # for i in range(s_dnn.nn1):
    #     ax[i, 0].plot(time[:-1], s_dnn_sigma[:, i])
    #     ax[i, 0].set_title(f'$\sigma_{{{i}}} (x) $')
    #     # ax[i, 0].set_xlim(0,xlim)
    #
    # # phi
    # for i in range(s_dnn.nn2):
    #     ax[i, 1].plot(time[:-1], s_dnn_phi_u[:, i])
    #     ax[i, 1].set_title(f'$\\varphi_{{{i}}} (x) u$')
    #     # ax[i, 1].set_xlim(0,xlim)
    #
    # plt.show()
